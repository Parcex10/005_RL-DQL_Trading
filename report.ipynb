{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce81576b",
   "metadata": {},
   "source": [
    "## 005 Reinsforment Leearning - Deep Q Learning Trading for Microsoft (MSFT)\n",
    "\n",
    "#### Autores: Adri√°n Herrera, Patrick F. B√°rcena y Carlos Moreno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0274f50e",
   "metadata": {},
   "source": [
    "### Marco Te√≥rico:\n",
    "\n",
    "El **`Aprendizaje por Refuerzo (Reinforcement Learning, RL)`** es un paradigma de aprendizaje autom√°tico en el que un agente aprende a tomar decisiones secuenciales a trav√©s de la interacci√≥n con un entorno din√°mico (ambiente). En lugar de utilizar datos etiquetados como en el aprendizaje supervisado, el agente explora distintas acciones, recibe recompensas o penalizaciones, y mejora progresivamente su estrategia para maximizar el retorno acumulado. \n",
    "\n",
    "Statquest hace un gran trabajo explicando esto, poniendo como ejemplo tomar la decisi√≥n probabil√≠stica de ir ya sea a Burguer King o M'cdonals por papas fritas, y que el desenlace positivo o negativo (reward) a la asisitencia aleatoria, ser√° la variable que determine que tan probable ser√° seguir asistiendo al lugar o no. \n",
    "\n",
    "En el contexto de **`trading algor√≠tmico`**, RL ofrece una forma poderosa de entrenar agentes que aprenden cu√°ndo **`comprar, vender o mantener`** activos financieros para optimizar m√©tricas como beneficios acumulados, Sharpe ratio o drawdowns.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **`¬øPor qu√© usar RL/DQL en trading?`**\n",
    "- üîÑ **Secuencialidad:** Permite al agente aprender decisiones encadenadas (*¬øvendo ahora o espero un d√≠a m√°s?*).  \n",
    "- üìà **Adaptabilidad:** Puede ajustarse a cambios din√°micos del mercado.  \n",
    "- üéØ **Exploraci√≥n vs explotaci√≥n:** Balancea entre probar estrategias nuevas y optimizar las conocidas.  \n",
    "\n",
    "---\n",
    "\n",
    "### üÜö **`Q-Learning vs Deep Q-Learning`**\n",
    "|                     | Q-Learning                        | Deep Q-Learning (DQL)           |\n",
    "|---------------------|-------------------------------------|-----------------------------------|\n",
    "| üî¢ **Representaci√≥n** | Tabla Q discreta                  | Red neuronal para estimar valores Q |\n",
    "| üß† **Escalabilidad**   | Limitada (no funciona bien con muchos estados) | Escalable a espacios de estados grandes |\n",
    "| üïí **Entrenamiento**   | R√°pido                            | M√°s pesado (requiere m√°s c√≥mputo) |\n",
    "| üìà **Aplicaci√≥n**      | Ambientes simples                 | Ambientes complejos (como trading real) |\n",
    "\n",
    "---\n",
    "\n",
    "### üí° **`Contexto del proyecto`**\n",
    "En este proyecto desarrollamos un agente de trading utilizando **Deep Q-Learning (DQL)** para aprender a operar sobre datos hist√≥ricos de **META (MSFT)**. Nuestro objetivo es evaluar c√≥mo un agente entrenado mediante RL se compara con una estrategia pasiva como *Buy & Hold* y analizar sus ventajas y limitaciones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed0951",
   "metadata": {},
   "source": [
    "### üìñ Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0603d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e25cd55",
   "metadata": {},
   "source": [
    "### üë®üèª‚Äçüíª Importaci√≥n de Datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "513594c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datos de MSFT guardados en data/MSFT_5yr.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Descargamos 5 a√±os de datos diarios\n",
    "df = yf.download(\"MSFT\", start=\"2018-01-01\", end=\"2023-12-31\")\n",
    "\n",
    "# Guardamos en CSV\n",
    "df.to_csv(\"data/MSFT_5yr.csv\")\n",
    "\n",
    "print(\"‚úÖ Datos de MSFT guardados en data/MSFT_5yr.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d342a295",
   "metadata": {},
   "source": [
    "### ‚öíÔ∏è Definici√≥n de Estados y Acciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37163d40",
   "metadata": {},
   "source": [
    "Acciones posibles (Action Space):\n",
    "\n",
    "0 = Hold (mantener)\n",
    "\n",
    "1 = Buy (comprar)\n",
    "\n",
    "2 = Sell (vender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bb38215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Estados y acciones definidos.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Close</th>\n",
       "      <th>Close_Normalized</th>\n",
       "      <th>Position</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>MSFT</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>79.328529</td>\n",
       "      <td>0.002892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>79.697716</td>\n",
       "      <td>0.004123</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>80.399185</td>\n",
       "      <td>0.006462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>81.395943</td>\n",
       "      <td>0.009785</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>81.479034</td>\n",
       "      <td>0.010062</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price           Close Close_Normalized Position\n",
       "Ticker           MSFT                          \n",
       "Date                                           \n",
       "2018-01-02  79.328529         0.002892        0\n",
       "2018-01-03  79.697716         0.004123        0\n",
       "2018-01-04  80.399185         0.006462        0\n",
       "2018-01-05  81.395943         0.009785        0\n",
       "2018-01-08  81.479034         0.010062        0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizamos precios de cierre entre 0 y 1\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[\"Close_Normalized\"] = scaler.fit_transform(df[[\"Close\"]])\n",
    "\n",
    "# A√±adimos columna de posici√≥n actual (inicialmente 0 = sin posici√≥n)\n",
    "df[\"Position\"] = 0\n",
    "\n",
    "# Definimos espacio de acciones\n",
    "actions = {0: \"Hold\", 1: \"Buy\", 2: \"Sell\"}\n",
    "\n",
    "print(\"‚úÖ Estados y acciones definidos.\")\n",
    "df[[\"Close\", \"Close_Normalized\", \"Position\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8498c17",
   "metadata": {},
   "source": [
    "### üß† Clase TradingEnvironment\n",
    "\n",
    "Creamos el simulador previo a la partici√≥n de los datos para el modelo tenga bases para jugar y experimentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633a32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnvironment:\n",
    "    def __init__(self, data, initial_balance=10000):\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.initial_balance = initial_balance\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = 0  # 0 = no posici√≥n, 1 = comprado\n",
    "        self.shares_held = 0\n",
    "        self.total_asset = self.balance\n",
    "        self.done = False\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        price = float(self.data.loc[self.current_step, \"Close_Normalized\"].iloc[0])\n",
    "        return np.array([price, self.position])\n",
    "\n",
    "    def step(self, action):\n",
    "        price = self.data.loc[self.current_step, \"Close\"]\n",
    "        reward = 0\n",
    "\n",
    "        if action == 1 and self.position == 0:  # Buy\n",
    "            self.shares_held = self.balance // price\n",
    "            self.balance -= self.shares_held * price\n",
    "            self.position = 1\n",
    "        elif action == 2 and self.position == 1:  # Sell\n",
    "            self.balance += self.shares_held * price\n",
    "            self.shares_held = 0\n",
    "            self.position = 0\n",
    "            reward = self.balance - self.initial_balance\n",
    "        else:\n",
    "            # Hold or invalid action\n",
    "            reward = 0\n",
    "\n",
    "        self.total_asset = self.balance + self.shares_held * price\n",
    "        self.current_step += 1\n",
    "\n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            self.done = True\n",
    "\n",
    "        return self._get_state(), reward, self.done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2279dc7b",
   "metadata": {},
   "source": [
    "### üèãüèº‚Äç‚ôÇÔ∏è  Entrenamiento del entorno (Q Learning Cl√°sico - prueba)\n",
    "\n",
    "Hacemos una prueba r√°pida del entorno con un algoritmo de Q-Learning cl√°sico para ver si funciona correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98bbce1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/qf312yqs5wnb29fvj3txfrqm0000gn/T/ipykernel_11018/1693972736.py:17: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  price = float(self.data.loc[self.current_step, \"Close_Normalized\"])\n",
      "/var/folders/zq/qf312yqs5wnb29fvj3txfrqm0000gn/T/ipykernel_11018/3837540229.py:35: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  q_table[state][action] = new_q\n",
      "/var/folders/zq/qf312yqs5wnb29fvj3txfrqm0000gn/T/ipykernel_11018/3837540229.py:44: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  print(f\"üéØ Episodio {ep+1}/{episodes} | Recompensa Total: {float(total_reward):.2f}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Episodio 1/100 | Recompensa Total: 1693134.77\n",
      "üéØ Episodio 2/100 | Recompensa Total: 1747670.17\n",
      "üéØ Episodio 3/100 | Recompensa Total: 1396757.35\n",
      "üéØ Episodio 4/100 | Recompensa Total: 2062276.27\n",
      "üéØ Episodio 5/100 | Recompensa Total: 3130830.21\n",
      "üéØ Episodio 6/100 | Recompensa Total: 3341704.05\n",
      "üéØ Episodio 7/100 | Recompensa Total: 1807261.99\n",
      "üéØ Episodio 8/100 | Recompensa Total: 545052.57\n",
      "üéØ Episodio 9/100 | Recompensa Total: 4566.41\n",
      "üéØ Episodio 10/100 | Recompensa Total: 3346237.19\n",
      "üéØ Episodio 11/100 | Recompensa Total: 2064567.73\n",
      "üéØ Episodio 12/100 | Recompensa Total: 4271816.61\n",
      "üéØ Episodio 13/100 | Recompensa Total: 2715026.75\n",
      "üéØ Episodio 14/100 | Recompensa Total: 1893919.75\n",
      "üéØ Episodio 15/100 | Recompensa Total: 3507437.13\n",
      "üéØ Episodio 16/100 | Recompensa Total: 1421627.44\n",
      "üéØ Episodio 17/100 | Recompensa Total: 196840.67\n",
      "üéØ Episodio 18/100 | Recompensa Total: 2290460.81\n",
      "üéØ Episodio 19/100 | Recompensa Total: 1092366.70\n",
      "üéØ Episodio 20/100 | Recompensa Total: 1812485.10\n",
      "üéØ Episodio 21/100 | Recompensa Total: 2140563.25\n",
      "üéØ Episodio 22/100 | Recompensa Total: 3696467.52\n",
      "üéØ Episodio 23/100 | Recompensa Total: 1740014.80\n",
      "üéØ Episodio 24/100 | Recompensa Total: 2838637.71\n",
      "üéØ Episodio 25/100 | Recompensa Total: 3417811.50\n",
      "üéØ Episodio 26/100 | Recompensa Total: 1960295.16\n",
      "üéØ Episodio 27/100 | Recompensa Total: 2985644.28\n",
      "üéØ Episodio 28/100 | Recompensa Total: 3773849.91\n",
      "üéØ Episodio 29/100 | Recompensa Total: 184824.78\n",
      "üéØ Episodio 30/100 | Recompensa Total: 1601648.48\n",
      "üéØ Episodio 31/100 | Recompensa Total: 1043970.30\n",
      "üéØ Episodio 32/100 | Recompensa Total: 3729469.19\n",
      "üéØ Episodio 33/100 | Recompensa Total: 1334050.37\n",
      "üéØ Episodio 34/100 | Recompensa Total: 467081.80\n",
      "üéØ Episodio 35/100 | Recompensa Total: 1534954.76\n",
      "üéØ Episodio 36/100 | Recompensa Total: 766901.71\n",
      "üéØ Episodio 37/100 | Recompensa Total: 988332.63\n",
      "üéØ Episodio 38/100 | Recompensa Total: 1009574.22\n",
      "üéØ Episodio 39/100 | Recompensa Total: 2395309.44\n",
      "üéØ Episodio 40/100 | Recompensa Total: 986491.49\n",
      "üéØ Episodio 41/100 | Recompensa Total: 5844470.12\n",
      "üéØ Episodio 42/100 | Recompensa Total: 2229258.23\n",
      "üéØ Episodio 43/100 | Recompensa Total: 3741061.90\n",
      "üéØ Episodio 44/100 | Recompensa Total: 1634794.99\n",
      "üéØ Episodio 45/100 | Recompensa Total: 3302557.51\n",
      "üéØ Episodio 46/100 | Recompensa Total: 3220972.20\n",
      "üéØ Episodio 47/100 | Recompensa Total: 1206330.54\n",
      "üéØ Episodio 48/100 | Recompensa Total: 2681711.47\n",
      "üéØ Episodio 49/100 | Recompensa Total: 4631048.60\n",
      "üéØ Episodio 50/100 | Recompensa Total: 1764539.44\n",
      "üéØ Episodio 51/100 | Recompensa Total: 1915186.17\n",
      "üéØ Episodio 52/100 | Recompensa Total: 3827890.81\n",
      "üéØ Episodio 53/100 | Recompensa Total: 2272051.30\n",
      "üéØ Episodio 54/100 | Recompensa Total: 880382.71\n",
      "üéØ Episodio 55/100 | Recompensa Total: 4492714.76\n",
      "üéØ Episodio 56/100 | Recompensa Total: 1477147.36\n",
      "üéØ Episodio 57/100 | Recompensa Total: 3015698.32\n",
      "üéØ Episodio 58/100 | Recompensa Total: 935986.92\n",
      "üéØ Episodio 59/100 | Recompensa Total: 1649557.36\n",
      "üéØ Episodio 60/100 | Recompensa Total: 1683340.77\n",
      "üéØ Episodio 61/100 | Recompensa Total: 3033449.72\n",
      "üéØ Episodio 62/100 | Recompensa Total: 4028031.61\n",
      "üéØ Episodio 63/100 | Recompensa Total: 1244380.04\n",
      "üéØ Episodio 64/100 | Recompensa Total: 2337045.25\n",
      "üéØ Episodio 65/100 | Recompensa Total: 3031344.77\n",
      "üéØ Episodio 66/100 | Recompensa Total: 6563263.47\n",
      "üéØ Episodio 67/100 | Recompensa Total: 4897365.46\n",
      "üéØ Episodio 68/100 | Recompensa Total: 4047204.01\n",
      "üéØ Episodio 69/100 | Recompensa Total: 4030508.42\n",
      "üéØ Episodio 70/100 | Recompensa Total: 3152439.77\n",
      "üéØ Episodio 71/100 | Recompensa Total: 5925221.55\n",
      "üéØ Episodio 72/100 | Recompensa Total: 843163.11\n",
      "üéØ Episodio 73/100 | Recompensa Total: 2418087.49\n",
      "üéØ Episodio 74/100 | Recompensa Total: 1799547.96\n",
      "üéØ Episodio 75/100 | Recompensa Total: 3038775.22\n",
      "üéØ Episodio 76/100 | Recompensa Total: 2275772.52\n",
      "üéØ Episodio 77/100 | Recompensa Total: 3631928.48\n",
      "üéØ Episodio 78/100 | Recompensa Total: 20808.88\n",
      "üéØ Episodio 79/100 | Recompensa Total: 2233769.80\n",
      "üéØ Episodio 80/100 | Recompensa Total: 1931511.05\n",
      "üéØ Episodio 81/100 | Recompensa Total: 1965907.48\n",
      "üéØ Episodio 82/100 | Recompensa Total: 6576357.25\n",
      "üéØ Episodio 83/100 | Recompensa Total: 5882400.48\n",
      "üéØ Episodio 84/100 | Recompensa Total: 1480592.19\n",
      "üéØ Episodio 85/100 | Recompensa Total: 5718211.04\n",
      "üéØ Episodio 86/100 | Recompensa Total: 2583607.82\n",
      "üéØ Episodio 87/100 | Recompensa Total: 618057.66\n",
      "üéØ Episodio 88/100 | Recompensa Total: 5908529.26\n",
      "üéØ Episodio 89/100 | Recompensa Total: 4777259.82\n",
      "üéØ Episodio 90/100 | Recompensa Total: 2681851.76\n",
      "üéØ Episodio 91/100 | Recompensa Total: 3237769.87\n",
      "üéØ Episodio 92/100 | Recompensa Total: 6395260.67\n",
      "üéØ Episodio 93/100 | Recompensa Total: 2152278.11\n",
      "üéØ Episodio 94/100 | Recompensa Total: 1562537.41\n",
      "üéØ Episodio 95/100 | Recompensa Total: 3918435.20\n",
      "üéØ Episodio 96/100 | Recompensa Total: 3937191.57\n",
      "üéØ Episodio 97/100 | Recompensa Total: 3793302.00\n",
      "üéØ Episodio 98/100 | Recompensa Total: 8182460.33\n",
      "üéØ Episodio 99/100 | Recompensa Total: 3414884.83\n",
      "üéØ Episodio 100/100 | Recompensa Total: 6853807.51\n",
      "‚úÖ Entrenamiento Q-Learning b√°sico completado.\n"
     ]
    }
   ],
   "source": [
    "# üìö Tabla Q inicial\n",
    "q_table = {}\n",
    "\n",
    "# Hiperpar√°metros\n",
    "alpha = 0.1\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.01\n",
    "episodes = 100\n",
    "\n",
    "env = TradingEnvironment(df)\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = tuple(env.reset())\n",
    "    total_reward = 0\n",
    "\n",
    "    while not env.done:\n",
    "        # Epsilon-greedy: Explorar o explotar\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = np.random.choice([0, 1, 2])  # Explorar\n",
    "        else:\n",
    "            action = q_table.get(state, np.zeros(3)).argmax()  # Explotar\n",
    "\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_state = tuple(next_state)\n",
    "\n",
    "        # Actualizar tabla Q\n",
    "        old_q = q_table.get(state, np.zeros(3))[action]\n",
    "        next_max = q_table.get(next_state, np.zeros(3)).max()\n",
    "\n",
    "        new_q = (1 - alpha) * old_q + alpha * (reward + gamma * next_max)\n",
    "\n",
    "        q_table.setdefault(state, np.zeros(3))\n",
    "        q_table[state][action] = new_q\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    # Decay de epsilon\n",
    "    if epsilon > min_epsilon:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    print(f\"üéØ Episodio {ep+1}/{episodes} | Recompensa Total: {float(total_reward):.2f}\")\n",
    "\n",
    "print(\"‚úÖ Entrenamiento Q-Learning b√°sico completado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde99e69",
   "metadata": {},
   "source": [
    "## üìä **An√°lisis de resultados: Q-Learning cl√°sico**\n",
    "\n",
    "Durante los 100 episodios de entrenamiento con el agente Q-Learning cl√°sico, observamos una evoluci√≥n interesante en las recompensas acumuladas:  \n",
    "\n",
    "  - Las recompensas muestran una alta variabilidad y algunos valores extremadamente bajos. Esto se debe a que el agente est√° en la fase de **exploraci√≥n (Œµ-greedy)**, probando acciones al azar para aprender sobre el entorno.  \n",
    "\n",
    "- **Mejora progresiva (Episodios 21-80):**\n",
    "  - Tambi√©n, mpiezan a aparecer episodios con recompensas significativamente m√°s altas (>3M), lo cual indica que el agente comienza a identificar **estrategias b√°sicas rentables**.  \n",
    "\n",
    "- \n",
    "  - Hacia los √∫ltimos episodios, se observa un aumento notable en las recompensas acumuladas (algunos episodios superan los 8M). Esto sugiere que el agente est√° **explotando mejor las pol√≠ticas aprendidas** para maximizar beneficios. Igual son solo 100 episodios, seguimos creciendo como agentes.\n",
    "\n",
    "\n",
    "###  **Pr√≥ximos pasos**\n",
    "En la siguiente fase desarrollaremos un **agente DQL con red neuronal**, lo que permitir√°:  \n",
    "- Aproximar funciones Q para espacios de estados continuos.  \n",
    "- Mejorar la capacidad de generalizaci√≥n del agente.  \n",
    "- Evaluar su rendimiento frente al Q-Learning cl√°sico y una estrategia *Buy & Hold*.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de380e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/qf312yqs5wnb29fvj3txfrqm0000gn/T/ipykernel_11018/1693972736.py:17: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  price = float(self.data.loc[self.current_step, \"Close_Normalized\"])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't assign a Series to a torch.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m         total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[0;32m---> 26\u001b[0m             \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müéØ Episodio \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Recompensa Total: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Entrenamiento DQL (versi√≥n r√°pida) completado.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Verano/Trading/Projects/005_RL-DQL_Trading/utils/rl_agent.py:61\u001b[0m, in \u001b[0;36mDQLAgent.replay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     59\u001b[0m target_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(state)\n\u001b[1;32m     60\u001b[0m target_f \u001b[38;5;241m=\u001b[39m target_f\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m---> 61\u001b[0m \u001b[43mtarget_f\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m target\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Optimize the model\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mTypeError\u001b[0m: can't assign a Series to a torch.FloatTensor"
     ]
    }
   ],
   "source": [
    "# üì• Importar el agente DQL desde utils\n",
    "from utils.rl_agent import DQLAgent\n",
    "\n",
    "# üì¶ Par√°metros\n",
    "state_size = 2  # [precio_normalizado, posici√≥n]\n",
    "action_size = 3  # Buy, Sell, Hold\n",
    "agent = DQLAgent(state_size, action_size)\n",
    "episodes = 100\n",
    "batch_size = 32\n",
    "\n",
    "env = TradingEnvironment(df)\n",
    "\n",
    "# üöÄ Entrenamiento\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while not env.done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "    print(f\"üéØ Episodio {ep+1}/{episodes} | Recompensa Total: {total_reward:.2f}\")\n",
    "\n",
    "print(\"‚úÖ Entrenamiento DQL (versi√≥n r√°pida) completado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c966c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18963470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4805a5a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
