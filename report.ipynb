{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce81576b",
   "metadata": {},
   "source": [
    "## 005 Reinsforment Leearning - Deep Q Learning Trading for Microsoft (MSFT)\n",
    "\n",
    "#### Autores: AdriÃ¡n Herrera, Patrick F. BÃ¡rcena y Carlos Moreno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0274f50e",
   "metadata": {},
   "source": [
    "### Marco TeÃ³rico:\n",
    "\n",
    "El **`Aprendizaje por Refuerzo (Reinforcement Learning, RL)`** es un paradigma de aprendizaje automÃ¡tico en el que un agente aprende a tomar decisiones secuenciales a travÃ©s de la interacciÃ³n con un entorno dinÃ¡mico (ambiente). En lugar de utilizar datos etiquetados como en el aprendizaje supervisado, el agente explora distintas acciones, recibe recompensas o penalizaciones, y mejora progresivamente su estrategia para maximizar el retorno acumulado. \n",
    "\n",
    "Statquest hace un gran trabajo explicando esto, poniendo como ejemplo tomar la decisiÃ³n probabilÃ­stica de ir ya sea a Burguer King o M'cdonals por papas fritas, y que el desenlace positivo o negativo (reward) a la asisitencia aleatoria, serÃ¡ la variable que determine que tan probable serÃ¡ seguir asistiendo al lugar o no. \n",
    "\n",
    "En el contexto de **`trading algorÃ­tmico`**, RL ofrece una forma poderosa de entrenar agentes que aprenden cuÃ¡ndo **`comprar, vender o mantener`** activos financieros para optimizar mÃ©tricas como beneficios acumulados, Sharpe ratio o drawdowns.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **`Â¿Por quÃ© usar RL/DQL en trading?`**\n",
    "- ğŸ”„ **Secuencialidad:** Permite al agente aprender decisiones encadenadas (*Â¿vendo ahora o espero un dÃ­a mÃ¡s?*).  \n",
    "- ğŸ“ˆ **Adaptabilidad:** Puede ajustarse a cambios dinÃ¡micos del mercado.  \n",
    "- ğŸ¯ **ExploraciÃ³n vs explotaciÃ³n:** Balancea entre probar estrategias nuevas y optimizar las conocidas.  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ†š **`Q-Learning vs Deep Q-Learning`**\n",
    "|                     | Q-Learning                        | Deep Q-Learning (DQL)           |\n",
    "|---------------------|-------------------------------------|-----------------------------------|\n",
    "| ğŸ”¢ **RepresentaciÃ³n** | Tabla Q discreta                  | Red neuronal para estimar valores Q |\n",
    "| ğŸ§  **Escalabilidad**   | Limitada (no funciona bien con muchos estados) | Escalable a espacios de estados grandes |\n",
    "| ğŸ•’ **Entrenamiento**   | RÃ¡pido                            | MÃ¡s pesado (requiere mÃ¡s cÃ³mputo) |\n",
    "| ğŸ“ˆ **AplicaciÃ³n**      | Ambientes simples                 | Ambientes complejos (como trading real) |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ **`Contexto del proyecto`**\n",
    "En este proyecto desarrollamos un agente de trading utilizando **Deep Q-Learning (DQL)** para aprender a operar sobre datos histÃ³ricos de **META (MSFT)**. Nuestro objetivo es evaluar cÃ³mo un agente entrenado mediante RL se compara con una estrategia pasiva como *Buy & Hold* y analizar sus ventajas y limitaciones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed0951",
   "metadata": {},
   "source": [
    "### ğŸ“– LibrerÃ­as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0603d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e25cd55",
   "metadata": {},
   "source": [
    "### ğŸ‘¨ğŸ»â€ğŸ’» ImportaciÃ³n de Datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "513594c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Datos de MSFT guardados en data/MSFT_5yr.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Descargamos 5 aÃ±os de datos diarios\n",
    "df = yf.download(\"MSFT\", start=\"2018-01-01\", end=\"2023-12-31\")\n",
    "\n",
    "# Guardamos en CSV\n",
    "df.to_csv(\"data/MSFT_5yr.csv\")\n",
    "\n",
    "print(\"âœ… Datos de MSFT guardados en data/MSFT_5yr.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d342a295",
   "metadata": {},
   "source": [
    "### âš’ï¸ DefiniciÃ³n de Estados y Acciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37163d40",
   "metadata": {},
   "source": [
    "Acciones posibles (Action Space):\n",
    "\n",
    "0 = Hold (mantener)\n",
    "\n",
    "1 = Buy (comprar)\n",
    "\n",
    "2 = Sell (vender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bb38215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Estados y acciones definidos.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Close</th>\n",
       "      <th>Close_Normalized</th>\n",
       "      <th>Position</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>MSFT</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>79.328529</td>\n",
       "      <td>0.002892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>79.697716</td>\n",
       "      <td>0.004123</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>80.399185</td>\n",
       "      <td>0.006462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>81.395943</td>\n",
       "      <td>0.009785</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>81.479034</td>\n",
       "      <td>0.010062</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price           Close Close_Normalized Position\n",
       "Ticker           MSFT                          \n",
       "Date                                           \n",
       "2018-01-02  79.328529         0.002892        0\n",
       "2018-01-03  79.697716         0.004123        0\n",
       "2018-01-04  80.399185         0.006462        0\n",
       "2018-01-05  81.395943         0.009785        0\n",
       "2018-01-08  81.479034         0.010062        0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizamos precios de cierre entre 0 y 1\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[\"Close_Normalized\"] = scaler.fit_transform(df[[\"Close\"]])\n",
    "\n",
    "# AÃ±adimos columna de posiciÃ³n actual (inicialmente 0 = sin posiciÃ³n)\n",
    "df[\"Position\"] = 0\n",
    "\n",
    "# Definimos espacio de acciones\n",
    "actions = {0: \"Hold\", 1: \"Buy\", 2: \"Sell\"}\n",
    "\n",
    "print(\"âœ… Estados y acciones definidos.\")\n",
    "df[[\"Close\", \"Close_Normalized\", \"Position\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8498c17",
   "metadata": {},
   "source": [
    "### ğŸ§  Clase TradingEnvironment\n",
    "\n",
    "Creamos el simulador previo a la particiÃ³n de los datos para el modelo tenga bases para jugar y experimentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633a32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnvironment:\n",
    "    def __init__(self, data, initial_balance=10000):\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.initial_balance = initial_balance\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = 0  # 0 = no posiciÃ³n, 1 = comprado\n",
    "        self.shares_held = 0\n",
    "        self.total_asset = self.balance\n",
    "        self.done = False\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        price = float(self.data.loc[self.current_step, \"Close_Normalized\"].iloc[0])\n",
    "        return np.array([price, self.position])\n",
    "\n",
    "    def step(self, action):\n",
    "        price = self.data.loc[self.current_step, \"Close\"]\n",
    "        reward = 0\n",
    "\n",
    "        if action == 1 and self.position == 0:  # Buy\n",
    "            self.shares_held = self.balance // price\n",
    "            self.balance -= self.shares_held * price\n",
    "            self.position = 1\n",
    "        elif action == 2 and self.position == 1:  # Sell\n",
    "            self.balance += self.shares_held * price\n",
    "            self.shares_held = 0\n",
    "            self.position = 0\n",
    "            reward = self.balance - self.initial_balance\n",
    "        else:\n",
    "            # Hold or invalid action\n",
    "            reward = 0\n",
    "\n",
    "        self.total_asset = self.balance + self.shares_held * price\n",
    "        self.current_step += 1\n",
    "\n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            self.done = True\n",
    "\n",
    "        return self._get_state(), reward, self.done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2279dc7b",
   "metadata": {},
   "source": [
    "### ğŸ‹ğŸ¼â€â™‚ï¸  Entrenamiento del entorno (Q Learning ClÃ¡sico - prueba)\n",
    "\n",
    "Hacemos una prueba rÃ¡pida del entorno con un algoritmo de Q-Learning clÃ¡sico para ver si funciona correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98bbce1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/qf312yqs5wnb29fvj3txfrqm0000gn/T/ipykernel_11018/1693972736.py:17: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  price = float(self.data.loc[self.current_step, \"Close_Normalized\"])\n",
      "/var/folders/zq/qf312yqs5wnb29fvj3txfrqm0000gn/T/ipykernel_11018/3837540229.py:35: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  q_table[state][action] = new_q\n",
      "/var/folders/zq/qf312yqs5wnb29fvj3txfrqm0000gn/T/ipykernel_11018/3837540229.py:44: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  print(f\"ğŸ¯ Episodio {ep+1}/{episodes} | Recompensa Total: {float(total_reward):.2f}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Episodio 1/100 | Recompensa Total: 1693134.77\n",
      "ğŸ¯ Episodio 2/100 | Recompensa Total: 1747670.17\n",
      "ğŸ¯ Episodio 3/100 | Recompensa Total: 1396757.35\n",
      "ğŸ¯ Episodio 4/100 | Recompensa Total: 2062276.27\n",
      "ğŸ¯ Episodio 5/100 | Recompensa Total: 3130830.21\n",
      "ğŸ¯ Episodio 6/100 | Recompensa Total: 3341704.05\n",
      "ğŸ¯ Episodio 7/100 | Recompensa Total: 1807261.99\n",
      "ğŸ¯ Episodio 8/100 | Recompensa Total: 545052.57\n",
      "ğŸ¯ Episodio 9/100 | Recompensa Total: 4566.41\n",
      "ğŸ¯ Episodio 10/100 | Recompensa Total: 3346237.19\n",
      "ğŸ¯ Episodio 11/100 | Recompensa Total: 2064567.73\n",
      "ğŸ¯ Episodio 12/100 | Recompensa Total: 4271816.61\n",
      "ğŸ¯ Episodio 13/100 | Recompensa Total: 2715026.75\n",
      "ğŸ¯ Episodio 14/100 | Recompensa Total: 1893919.75\n",
      "ğŸ¯ Episodio 15/100 | Recompensa Total: 3507437.13\n",
      "ğŸ¯ Episodio 16/100 | Recompensa Total: 1421627.44\n",
      "ğŸ¯ Episodio 17/100 | Recompensa Total: 196840.67\n",
      "ğŸ¯ Episodio 18/100 | Recompensa Total: 2290460.81\n",
      "ğŸ¯ Episodio 19/100 | Recompensa Total: 1092366.70\n",
      "ğŸ¯ Episodio 20/100 | Recompensa Total: 1812485.10\n",
      "ğŸ¯ Episodio 21/100 | Recompensa Total: 2140563.25\n",
      "ğŸ¯ Episodio 22/100 | Recompensa Total: 3696467.52\n",
      "ğŸ¯ Episodio 23/100 | Recompensa Total: 1740014.80\n",
      "ğŸ¯ Episodio 24/100 | Recompensa Total: 2838637.71\n",
      "ğŸ¯ Episodio 25/100 | Recompensa Total: 3417811.50\n",
      "ğŸ¯ Episodio 26/100 | Recompensa Total: 1960295.16\n",
      "ğŸ¯ Episodio 27/100 | Recompensa Total: 2985644.28\n",
      "ğŸ¯ Episodio 28/100 | Recompensa Total: 3773849.91\n",
      "ğŸ¯ Episodio 29/100 | Recompensa Total: 184824.78\n",
      "ğŸ¯ Episodio 30/100 | Recompensa Total: 1601648.48\n",
      "ğŸ¯ Episodio 31/100 | Recompensa Total: 1043970.30\n",
      "ğŸ¯ Episodio 32/100 | Recompensa Total: 3729469.19\n",
      "ğŸ¯ Episodio 33/100 | Recompensa Total: 1334050.37\n",
      "ğŸ¯ Episodio 34/100 | Recompensa Total: 467081.80\n",
      "ğŸ¯ Episodio 35/100 | Recompensa Total: 1534954.76\n",
      "ğŸ¯ Episodio 36/100 | Recompensa Total: 766901.71\n",
      "ğŸ¯ Episodio 37/100 | Recompensa Total: 988332.63\n",
      "ğŸ¯ Episodio 38/100 | Recompensa Total: 1009574.22\n",
      "ğŸ¯ Episodio 39/100 | Recompensa Total: 2395309.44\n",
      "ğŸ¯ Episodio 40/100 | Recompensa Total: 986491.49\n",
      "ğŸ¯ Episodio 41/100 | Recompensa Total: 5844470.12\n",
      "ğŸ¯ Episodio 42/100 | Recompensa Total: 2229258.23\n",
      "ğŸ¯ Episodio 43/100 | Recompensa Total: 3741061.90\n",
      "ğŸ¯ Episodio 44/100 | Recompensa Total: 1634794.99\n",
      "ğŸ¯ Episodio 45/100 | Recompensa Total: 3302557.51\n",
      "ğŸ¯ Episodio 46/100 | Recompensa Total: 3220972.20\n",
      "ğŸ¯ Episodio 47/100 | Recompensa Total: 1206330.54\n",
      "ğŸ¯ Episodio 48/100 | Recompensa Total: 2681711.47\n",
      "ğŸ¯ Episodio 49/100 | Recompensa Total: 4631048.60\n",
      "ğŸ¯ Episodio 50/100 | Recompensa Total: 1764539.44\n",
      "ğŸ¯ Episodio 51/100 | Recompensa Total: 1915186.17\n",
      "ğŸ¯ Episodio 52/100 | Recompensa Total: 3827890.81\n",
      "ğŸ¯ Episodio 53/100 | Recompensa Total: 2272051.30\n",
      "ğŸ¯ Episodio 54/100 | Recompensa Total: 880382.71\n",
      "ğŸ¯ Episodio 55/100 | Recompensa Total: 4492714.76\n",
      "ğŸ¯ Episodio 56/100 | Recompensa Total: 1477147.36\n",
      "ğŸ¯ Episodio 57/100 | Recompensa Total: 3015698.32\n",
      "ğŸ¯ Episodio 58/100 | Recompensa Total: 935986.92\n",
      "ğŸ¯ Episodio 59/100 | Recompensa Total: 1649557.36\n",
      "ğŸ¯ Episodio 60/100 | Recompensa Total: 1683340.77\n",
      "ğŸ¯ Episodio 61/100 | Recompensa Total: 3033449.72\n",
      "ğŸ¯ Episodio 62/100 | Recompensa Total: 4028031.61\n",
      "ğŸ¯ Episodio 63/100 | Recompensa Total: 1244380.04\n",
      "ğŸ¯ Episodio 64/100 | Recompensa Total: 2337045.25\n",
      "ğŸ¯ Episodio 65/100 | Recompensa Total: 3031344.77\n",
      "ğŸ¯ Episodio 66/100 | Recompensa Total: 6563263.47\n",
      "ğŸ¯ Episodio 67/100 | Recompensa Total: 4897365.46\n",
      "ğŸ¯ Episodio 68/100 | Recompensa Total: 4047204.01\n",
      "ğŸ¯ Episodio 69/100 | Recompensa Total: 4030508.42\n",
      "ğŸ¯ Episodio 70/100 | Recompensa Total: 3152439.77\n",
      "ğŸ¯ Episodio 71/100 | Recompensa Total: 5925221.55\n",
      "ğŸ¯ Episodio 72/100 | Recompensa Total: 843163.11\n",
      "ğŸ¯ Episodio 73/100 | Recompensa Total: 2418087.49\n",
      "ğŸ¯ Episodio 74/100 | Recompensa Total: 1799547.96\n",
      "ğŸ¯ Episodio 75/100 | Recompensa Total: 3038775.22\n",
      "ğŸ¯ Episodio 76/100 | Recompensa Total: 2275772.52\n",
      "ğŸ¯ Episodio 77/100 | Recompensa Total: 3631928.48\n",
      "ğŸ¯ Episodio 78/100 | Recompensa Total: 20808.88\n",
      "ğŸ¯ Episodio 79/100 | Recompensa Total: 2233769.80\n",
      "ğŸ¯ Episodio 80/100 | Recompensa Total: 1931511.05\n",
      "ğŸ¯ Episodio 81/100 | Recompensa Total: 1965907.48\n",
      "ğŸ¯ Episodio 82/100 | Recompensa Total: 6576357.25\n",
      "ğŸ¯ Episodio 83/100 | Recompensa Total: 5882400.48\n",
      "ğŸ¯ Episodio 84/100 | Recompensa Total: 1480592.19\n",
      "ğŸ¯ Episodio 85/100 | Recompensa Total: 5718211.04\n",
      "ğŸ¯ Episodio 86/100 | Recompensa Total: 2583607.82\n",
      "ğŸ¯ Episodio 87/100 | Recompensa Total: 618057.66\n",
      "ğŸ¯ Episodio 88/100 | Recompensa Total: 5908529.26\n",
      "ğŸ¯ Episodio 89/100 | Recompensa Total: 4777259.82\n",
      "ğŸ¯ Episodio 90/100 | Recompensa Total: 2681851.76\n",
      "ğŸ¯ Episodio 91/100 | Recompensa Total: 3237769.87\n",
      "ğŸ¯ Episodio 92/100 | Recompensa Total: 6395260.67\n",
      "ğŸ¯ Episodio 93/100 | Recompensa Total: 2152278.11\n",
      "ğŸ¯ Episodio 94/100 | Recompensa Total: 1562537.41\n",
      "ğŸ¯ Episodio 95/100 | Recompensa Total: 3918435.20\n",
      "ğŸ¯ Episodio 96/100 | Recompensa Total: 3937191.57\n",
      "ğŸ¯ Episodio 97/100 | Recompensa Total: 3793302.00\n",
      "ğŸ¯ Episodio 98/100 | Recompensa Total: 8182460.33\n",
      "ğŸ¯ Episodio 99/100 | Recompensa Total: 3414884.83\n",
      "ğŸ¯ Episodio 100/100 | Recompensa Total: 6853807.51\n",
      "âœ… Entrenamiento Q-Learning bÃ¡sico completado.\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“š Tabla Q inicial\n",
    "q_table = {}\n",
    "\n",
    "# HiperparÃ¡metros\n",
    "alpha = 0.1\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.01\n",
    "episodes = 100\n",
    "\n",
    "env = TradingEnvironment(df)\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = tuple(env.reset())\n",
    "    total_reward = 0\n",
    "\n",
    "    while not env.done:\n",
    "        # Epsilon-greedy: Explorar o explotar\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = np.random.choice([0, 1, 2])  # Explorar\n",
    "        else:\n",
    "            action = q_table.get(state, np.zeros(3)).argmax()  # Explotar\n",
    "\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_state = tuple(next_state)\n",
    "\n",
    "        # Actualizar tabla Q\n",
    "        old_q = q_table.get(state, np.zeros(3))[action]\n",
    "        next_max = q_table.get(next_state, np.zeros(3)).max()\n",
    "\n",
    "        new_q = (1 - alpha) * old_q + alpha * (reward + gamma * next_max)\n",
    "\n",
    "        q_table.setdefault(state, np.zeros(3))\n",
    "        q_table[state][action] = new_q\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    # Decay de epsilon\n",
    "    if epsilon > min_epsilon:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    print(f\"ğŸ¯ Episodio {ep+1}/{episodes} | Recompensa Total: {float(total_reward):.2f}\")\n",
    "\n",
    "print(\"âœ… Entrenamiento Q-Learning bÃ¡sico completado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde99e69",
   "metadata": {},
   "source": [
    "## ğŸ“Š **AnÃ¡lisis de resultados: Q-Learning clÃ¡sico**\n",
    "\n",
    "Durante los 100 episodios de entrenamiento con el agente Q-Learning clÃ¡sico, observamos una evoluciÃ³n interesante en las recompensas acumuladas:  \n",
    "\n",
    "  - Las recompensas muestran una alta variabilidad y algunos valores extremadamente bajos. Esto se debe a que el agente estÃ¡ en la fase de **exploraciÃ³n (Îµ-greedy)**, probando acciones al azar para aprender sobre el entorno.  \n",
    "\n",
    "- **Mejora progresiva (Episodios 21-80):**\n",
    "  - TambiÃ©n, mpiezan a aparecer episodios con recompensas significativamente mÃ¡s altas (>3M), lo cual indica que el agente comienza a identificar **estrategias bÃ¡sicas rentables**.  \n",
    "\n",
    "- \n",
    "  - Hacia los Ãºltimos episodios, se observa un aumento notable en las recompensas acumuladas (algunos episodios superan los 8M). Esto sugiere que el agente estÃ¡ **explotando mejor las polÃ­ticas aprendidas** para maximizar beneficios. Igual son solo 100 episodios, seguimos creciendo como agentes.\n",
    "\n",
    "\n",
    "###  **PrÃ³ximos pasos**\n",
    "En la siguiente fase desarrollaremos un **agente DQL con red neuronal**, lo que permitirÃ¡:  \n",
    "- Aproximar funciones Q para espacios de estados continuos.  \n",
    "- Mejorar la capacidad de generalizaciÃ³n del agente.  \n",
    "- Evaluar su rendimiento frente al Q-Learning clÃ¡sico y una estrategia *Buy & Hold*.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de380e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/qf312yqs5wnb29fvj3txfrqm0000gn/T/ipykernel_11018/1693972736.py:17: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  price = float(self.data.loc[self.current_step, \"Close_Normalized\"])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't assign a Series to a torch.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m         total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[0;32m---> 26\u001b[0m             \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ¯ Episodio \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Recompensa Total: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Entrenamiento DQL (versiÃ³n rÃ¡pida) completado.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Verano/Trading/Projects/005_RL-DQL_Trading/utils/rl_agent.py:61\u001b[0m, in \u001b[0;36mDQLAgent.replay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     59\u001b[0m target_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(state)\n\u001b[1;32m     60\u001b[0m target_f \u001b[38;5;241m=\u001b[39m target_f\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m---> 61\u001b[0m \u001b[43mtarget_f\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m target\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Optimize the model\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mTypeError\u001b[0m: can't assign a Series to a torch.FloatTensor"
     ]
    }
   ],
   "source": [
    "# ğŸ“¥ Importar el agente DQL desde utils\n",
    "from utils.rl_agent import DQLAgent\n",
    "\n",
    "# ğŸ“¦ ParÃ¡metros\n",
    "state_size = 2  # [precio_normalizado, posiciÃ³n]\n",
    "action_size = 3  # Buy, Sell, Hold\n",
    "agent = DQLAgent(state_size, action_size)\n",
    "episodes = 100\n",
    "batch_size = 32\n",
    "\n",
    "env = TradingEnvironment(df)\n",
    "\n",
    "# ğŸš€ Entrenamiento\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while not env.done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "    print(f\"ğŸ¯ Episodio {ep+1}/{episodes} | Recompensa Total: {total_reward:.2f}\")\n",
    "\n",
    "print(\"âœ… Entrenamiento DQL (versiÃ³n rÃ¡pida) completado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c966c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18963470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4805a5a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
