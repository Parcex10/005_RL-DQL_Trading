{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce81576b",
   "metadata": {},
   "source": [
    "## 005 Reinsforment Leearning - Deep Q Learning Trading for Microsoft (MSFT)\n",
    "\n",
    "Autores: AdriÃ¡n Herrera, Patrick F. BÃ¡rcena y Carlos Moreno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0274f50e",
   "metadata": {},
   "source": [
    "### Marco TeÃ³rico:\n",
    "\n",
    "El **`Aprendizaje por Refuerzo (Reinforcement Learning, RL)`** es un paradigma de aprendizaje automÃ¡tico en el que un agente aprende a tomar decisiones secuenciales a travÃ©s de la interacciÃ³n con un entorno dinÃ¡mico (ambiente). En lugar de utilizar datos etiquetados como en el aprendizaje supervisado, el agente explora distintas acciones, recibe recompensas o penalizaciones, y mejora progresivamente su estrategia para maximizar el retorno acumulado. \n",
    "\n",
    "Statquest hace un gran trabajo explicando esto, poniendo como ejemplo tomar la decisiÃ³n probabilÃ­stica de ir ya sea a Burguer King o M'cdonals por papas fritas, y que el desenlace positivo o negativo (reward) a la asisitencia aleatoria, serÃ¡ la variable que determine que tan probable serÃ¡ seguir asistiendo al lugar o no. \n",
    "\n",
    "En el contexto de **`trading algorÃ­tmico`**, RL ofrece una forma poderosa de entrenar agentes que aprenden cuÃ¡ndo **`comprar, vender o mantener`** activos financieros para optimizar mÃ©tricas como beneficios acumulados, Sharpe ratio o drawdowns.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **`Â¿Por quÃ© usar RL/DQL en trading?`**\n",
    "- ðŸ”„ **Secuencialidad:** Permite al agente aprender decisiones encadenadas (*Â¿vendo ahora o espero un dÃ­a mÃ¡s?*).  \n",
    "- ðŸ“ˆ **Adaptabilidad:** Puede ajustarse a cambios dinÃ¡micos del mercado.  \n",
    "- ðŸŽ¯ **ExploraciÃ³n vs explotaciÃ³n:** Balancea entre probar estrategias nuevas y optimizar las conocidas.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ†š **`Q-Learning vs Deep Q-Learning`**\n",
    "|                     | Q-Learning                        | Deep Q-Learning (DQL)           |\n",
    "|---------------------|-------------------------------------|-----------------------------------|\n",
    "| ðŸ”¢ **RepresentaciÃ³n** | Tabla Q discreta                  | Red neuronal para estimar valores Q |\n",
    "| ðŸ§  **Escalabilidad**   | Limitada (no funciona bien con muchos estados) | Escalable a espacios de estados grandes |\n",
    "| ðŸ•’ **Entrenamiento**   | RÃ¡pido                            | MÃ¡s pesado (requiere mÃ¡s cÃ³mputo) |\n",
    "| ðŸ“ˆ **AplicaciÃ³n**      | Ambientes simples                 | Ambientes complejos (como trading real) |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ **`Contexto del proyecto`**\n",
    "En este proyecto desarrollamos un agente de trading utilizando **Deep Q-Learning (DQL)** para aprender a operar sobre datos histÃ³ricos de **META (MSFT)**. Nuestro objetivo es evaluar cÃ³mo un agente entrenado mediante RL se compara con una estrategia pasiva como *Buy & Hold* y analizar sus ventajas y limitaciones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed0951",
   "metadata": {},
   "source": [
    "### ðŸ“– LibrerÃ­as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603d44d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513594c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d342a295",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
